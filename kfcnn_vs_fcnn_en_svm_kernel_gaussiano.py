# -*- coding: utf-8 -*-
"""kfcnn_vs_fcnn_en_SVM_kernel_gaussiano.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UxXMHHdntKdbhDYK-INkmo4T2rPBqhEQ
"""



import numpy as np
import time
from tqdm import *
import pandas as pd
from sklearn.preprocessing import LabelEncoder 
from time import process_time
from sklearn.datasets import load_digits
from sklearn.decomposition import KernelPCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_moons, make_circles
from sklearn.metrics import f1_score , accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
from sklearn.datasets import load_iris
from sklearn.preprocessing import LabelEncoder 
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import f1_score , accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier 
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel
import csv
from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel,euclidean_distances
import warnings
warnings.filterwarnings("ignore")
from sklearn.neural_network import MLPClassifier

path_datos='/content/drive/MyDrive/datos_prueba/datos_medianos/shuttle_train.csv'

df=pd.read_csv(path_datos)
labelencoder= LabelEncoder()
df.iloc[:,-1] = labelencoder.fit_transform(df.iloc[:,-1]) 
data=list()
data={'data':np.array(df.iloc[:,:-1]),'target':np.array(df.iloc[:,-1])}
X = data['data'][:,:]
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X,y ,test_size=0.2, random_state=0)#

X_train.shape

def FCNN(X,y):
  """
  Funcion FCNN, tiene como objetivo reducir el numero de instancias para 
  entrenar un clasificador, bajo la idea de tener una perdida minima en el 
  rendimiento del mismo
x|  -Entrada- 
  X: Matriz de nxp tal que n es el numero de observaciones y p el numero de 
  caracteristicas
  y: Vector de etiquetas de longitud n correspondiente a cada una de las 
  observaciones de X.
  -Salida- 
  Retorna un subconjunto consistente reducido S = [S(X),s(y)] en forma de 
  Vector de Caracteristicas - Etiquetas de Clase, lo que tambien se conoce 
  como labeled point, la etiqueta se retorna en la ultima columna.
  """
              #Por defecto utilizaremos el parametro k=1, pero puede ser modificado
  parametro_k=1
  k = parametro_k
              #Escalamos los datos de la matriz X
#  datosTrain = scaler.fit_transform(X)
  datosTrain = (X)
  #Nota: Si no se desea escalar para este momento, simplemente #datosTrain = X#
  clasesTrain = y

  nClases = 0 #Contar el numero de clases diferentes que tenemos 
  for i in range(len(clasesTrain)):
    if clasesTrain[i]>nClases:
      nClases = clasesTrain[i];
  nClases+=1

              #Inicializar el vector nearest como -1's
  nearest = np.random.randint(1, size=(len(datosTrain),k))-1

              #Inicializamos al conjunto S como un conjunto vacio
              # y tamS es un contador del nunero de elementos en S
  MAX_VALUE= 1000000000 
  S = np.random.randint(1, size=(len(datosTrain)))+MAX_VALUE
  tamS = 0    

              #Inicializamos a dS como las observaciones mas cercanas 
              #a los centroides
  deltaS = []
  for i in range(int(nClases)):
    nCentroid = 0;
    centroid = np.zeros(len(datosTrain[0]))
    for j in range(len(datosTrain)):
      if clasesTrain[j]==i: 
        for l in range(len(datosTrain[j])):
          centroid[l] += datosTrain[j][l];
        nCentroid+=1;
    for j in range(len(centroid)):
      centroid[j] /= nCentroid
    pos = -1;
    minDist = MAX_VALUE
    for j in range(len(datosTrain)):
      if (clasesTrain[j]==i):
          dist = np.linalg.norm(centroid-datosTrain[j])
          if dist<minDist:
            minDist = dist
            pos = j
    if (pos>=0):
      deltaS.append(pos)       

              #Validacion de numero de clases diferentes
  if (nClases < 2):
      print("Todos los datos pertenecen a una unica clase");
      nClases = 1;      
      return np.append(X[int(deltaS[0]):int(deltaS[0]+1)],np.array(y[int(deltaS[0]):int(deltaS[0]+1)]).reshape(-1,1),axis=1)
      
            #Una vez inicializado deltaS, procedemos a buscar en cada iteración
            #los elementos de los vectores nearest y rep
  while (len(deltaS)>0):
    for i in range(len(deltaS)):
      S[tamS] = deltaS[i]
      tamS+=1
    S = np.sort(S)  
    rep = np.random.randint(1, size=len(datosTrain))-1
    for i in range(len(datosTrain)):
      if not(i in S):
        for j in range(len(deltaS)):
          insert = False
          for l in (l for l in range(len(nearest[i])) if not insert):
            if nearest[i][l]<0:
              nearest[i][l] = deltaS[j]
              insert = True
            else:
              if (np.linalg.norm(datosTrain[nearest[i][l]]-datosTrain[i]) >=  np.linalg.norm(datosTrain[deltaS[j]] - datosTrain[i] )):
                for m in range(k-1,l,-1): 
                  nearest[i][m] = nearest[i][m-1]
                nearest[i][l] = deltaS[j]
                insert = True
        votes = np.random.randint(1, size=int(nClases))
        for j in range(len(nearest[i])):
          if nearest[i][j] >= 0:
            votes[ int(clasesTrain[nearest[i][j]]) ] += 1
        max=votes[0]
        pos=0
        for j in range(0,len(votes)):
          if votes[j]>max:
            max = votes[j]
            pos = j
        if clasesTrain[i] != pos:
          for j in range(len(nearest[i])):
            if nearest[i][j] >=0:
              if rep[nearest[i][j]]<0:
                rep[nearest[i][j]]=i
              else:
                if (np.linalg.linalg.norm(datosTrain[nearest[i][j]]-datosTrain[i]) <= np.linalg.linalg.norm(datosTrain[nearest[i][j]] - datosTrain[rep[nearest[i][j]] ])):
                  rep[nearest[i][j]] = i
              
              #Una vez finalizado el calculo de elementos en T respecto a cada 
              #elemento del conjunto S, los candidatos en rep son examinados
              #tal que se agregan a delta S si el i'esimo elemento en rep fue declarado
              #y si ademas dicho elemento aun no ha sido nombrado como candidato por
              #otra observación [evitando de esta manera agregar a dicho candidato 2 veces]
    deltaS = []
    for i in range(tamS): #(rep[S[i]] in S)==False
      if (rep[S[i]]>=0 and not( rep[S[i]] in deltaS) ) :
        deltaS.append(rep[S[i]])
              #A partir de los indices almacenados en el vector S, y considerando 
              #ademas la longitud del mismo, aquellas observaciones en S, se recopilan 
              #tal que podamos obtener el conjunto de etiquetas S final.
  y_final = []
  X_final = []
  #datosTrain = scaler.inverse_transform(datosTrain)
  for j in range(tamS):
    y_final.append(clasesTrain[int(S[j]) ]) 
    X_final.append(datosTrain[int(S[j])])
  return np.append(X_final,np.array(y_final).reshape(-1,1),axis=1)

def KFCNN(X,y,parametro_gamma):  
  """
  Funcion KFCNN, tiene como objetivo reducir el numero de instancias considerando
  la distancia euclideana en un espacio de caracteristicas donde dichos datos
  sean linealmente separables, lo anterior se hace para posteriormente
  entrenar un clasificador, bajo la idea de tener una perdida minima en el 
  rendimiento del mismo
  -Entrada- 
  X: Matriz de nxp tal que n es el numero de observaciones y p el numero de 
  caracteristicas
  y: Vector de etiquetas de longitud n correspondiente a cada una de las 
  observaciones de X.
  Parametro_gamma: corresponde al parametro que utiliza la función kernel rbf 
  su rango es (0,+ inf )
  -Salida- 
  Retorna un subconjunto consistente reducido S = [S(X),s(y)] en forma de 
  Vector de Caracteristicas - Etiquetas de Clase, lo que tambien se conoce 
  como labeled point, la etiqueta se retorna en la ultima columna.
  """
              #Por defecto utilizaremos el parametro k=1, pero puede ser modificado
  parametro_k=1
  k = parametro_k
              #Escalamos los datos de la matriz X
  X_escalado = X#scaler.fit_transform(X)
              #Realizamos el calculo de las distancias en el espacio kernel
              #Nota: Si no se desea escalar para este momento, simplemente 
              ##K = rbf_kernel(X,gamma=parametro_gamma)
  K = rbf_kernel(X_escalado,gamma=parametro_gamma)
  t=np.diag(K).reshape(-1,1)@(np.repeat(1,len(K)).reshape(1,-1))
  dXij = (t+t.T-2*K)
              #dXij, es la matriz utilizada para hacer las comparaciones mas adelante
              

  datosTrain = X
  clasesTrain = y

  nClases = 0 #Contar el numero de clases diferentes que tenemos 
  for i in range(len(clasesTrain)):
    if clasesTrain[i]>nClases:
      nClases = clasesTrain[i];
  nClases+=1

              #Inicializar el vector nearest como -1's
  nearest = np.random.randint(1, size=(len(datosTrain),k))-1


              #Inicializamos al conjunto S como un conjunto vacio
              # y tamS es un contador del nunero de elementos en S
  MAX_VALUE= 1000000000 
  S = np.random.randint(1, size=(len(datosTrain)))+MAX_VALUE
  tamS = 0

              #Inicializamos a dS como las observaciones mas cercanas 
              #a los centroides
  deltaS = []
  for i in range(int(nClases)):
    nCentroid = 0;
    centroid = np.zeros(len(datosTrain[0]))
    for j in range(len(datosTrain)):
      if clasesTrain[j]==i: 
        for l in range(len(datosTrain[j])):
          centroid[l] += datosTrain[j][l];
        nCentroid+=1;
    for j in range(len(centroid)):
      centroid[j] /= nCentroid
    pos = -1;
    minDist = MAX_VALUE
    for j in range(len(datosTrain)):
      if (clasesTrain[j]==i):
          dist = np.linalg.norm(centroid-datosTrain[j])
          if dist<minDist:
            minDist = dist
            pos = j
    if (pos>=0):
      deltaS.append(pos)


              #Validacion de numero de clases diferentes
  if (nClases < 2):
      print("Todos los datos pertenecen a una unica clase");
      nClases = 1;      
      return np.append(X[int(deltaS[0]):int(deltaS[0]+1)],np.array(y[int(deltaS[0]):int(deltaS[0]+1)]).reshape(-1,1),axis=1)


            #Una vez inicializado deltaS, procedemos a buscar en cada iteración
            #los elementos de los vectores nearest y rep
  while (len(deltaS)>0):
    for i in range(len(deltaS)):
      S[tamS] = deltaS[i]
      tamS+=1
    S = np.sort(S) 
    rep = np.random.randint(1, size=(len(datosTrain)))-1
    for i in range(len(datosTrain)):
      if not(i in S):
        for j in range(len(deltaS)):
          insert = False
          for l in (l for l in range(len(nearest[i])) if not insert):
            if nearest[i][l]<0:
              nearest[i][l] = deltaS[j]
              insert = True
            else:
            #Linea que es sustituida respecto al codigo original
              if dXij[nearest[i][l],i] >= dXij[deltaS[j],i]:
                for m in range(k-1,l,-1): 
                  nearest[i][m] = nearest[i][m-1]
                nearest[i][l] = deltaS[j]
                insert = True

        votes = np.random.randint(1, size=int(nClases))
        for j in range(len(nearest[i])):
          if nearest[i][j] >= 0:
            votes[int(clasesTrain[nearest[i][j]])]+=1
        max=votes[0]
        pos=0
        for j in range(0,len(votes)):
          if votes[j]>max:
            max = votes[j]
            pos = j
        if clasesTrain[i] != pos:
          for j in range(len(nearest[i])):
            if nearest[i][j] >=0:
              if rep[nearest[i][j]]<0:
                rep[nearest[i][j]]=i
              else:
                #Cambio respecto al codigo original
                if dXij[nearest[i][j], i] <= dXij[nearest[i][j],rep[nearest[i][j]]]:
                  rep[nearest[i][j]] = i

              #Una vez finalizado el calculo de elementos en T respecto a cada 
              #elemento del conjunto S, los candidatos en rep son examinados
              #tal que se agregan a delta S si el i'esimo elemento en rep fue declarado
              #y si ademas dicho elemento aun no ha sido nombrado como candidato por
              #otra observación [evitando de esta manera agregar a dicho candidato 2 veces]
    deltaS = []
    for i in range(tamS): #(rep[S[i]] in S)==False
      if (rep[S[i]]>=0 and not( rep[S[i]] in deltaS) ) :
        deltaS.append(rep[S[i]])
              #A partir de los indices almacenados en el vector S, y considerando 
              #ademas la longitud del mismo, aquellas observaciones en S, se recopilan 
              #tal que podamos obtener el conjunto de etiquetas S final.
  y_final = []
  X_final = []
  for j in range(tamS):
    y_final.append(clasesTrain[S[j]]) 
    X_final.append(datosTrain[S[j]])
  return np.append(X_final,np.array(y_final).reshape(-1,1),axis=1)





##El experimento que realizado hasta este momento es el siguiente:
"""
"""

###Grid Search
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
parameters = {'kernel':['rbf'], 'gamma':[1, 0.1, 0.01, 0.001, 0.0001]}
svc = svm.SVC()
clasificador = GridSearchCV(svc, parameters,scoring='f1_macro',n_jobs=-1)
clasificador.fit(X_train, y_train)
y_predicho = clasificador.predict(X_test)
###Rendimiento con conjunto de datos train
print(f1_score(y_predicho,y_test,average='macro'))
t_rbf = f1_score(y_predicho,y_test,average='macro')
clf= svm.SVC(kernel='linear').fit(X_train,y_train)
y_predicho = clf.predict(X_test)
print('linear,linear')
print(f1_score(y_predicho,y_test,average='macro'))
t_lineal = f1_score(y_predicho,y_test,average='macro')

print(clasificador.best_params_['gamma'])

###Rendimiento del subconjunto S, FCNN
inicio = time.time()
Conjunto_S = FCNN(X_train[:,:],y_train)
final = time.time()
print('Tiempo de ejecucion : ', final-inicio, 'Segundos')
print(Conjunto_S.shape)
X_red = Conjunto_S[:,:-1]
y_red = Conjunto_S[:,-1]
clf= svm.SVC(kernel='rbf',gamma=clasificador.best_params_['gamma']).fit(X_red,y_red)
y_predicho = clf.predict(X_test)
print('rbf,rbf')
print(f1_score(y_predicho,y_test,average='macro'))
fcnn_rbf = f1_score(y_predicho,y_test,average='macro')
clf= svm.SVC(kernel='linear').fit(X_red,y_red)
y_predicho = clf.predict(X_test)
print('linear,linear')
print(f1_score(y_predicho,y_test,average='macro'))
fcnn_lineal = f1_score(y_predicho,y_test,average='macro')

evolucion_linear = []
evolucion_poly = []
parametro_gamma = [0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5]#0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5
for i in parametro_gamma:
  inicio = time.time()
  Conjunto_S = KFCNN(X_train[:,:],y_train,parametro_gamma=i)
  final = time.time()
  print('inicia nueva ejecucion con kernel ',i)
  print('Tiempo de ejecucion : ', final-inicio, 'Segundos')
  print(Conjunto_S.shape)
  X_red = Conjunto_S[:,:-1]
  y_red = Conjunto_S[:,-1]
  clf= svm.SVC(kernel='rbf',gamma=clasificador.best_params_['gamma']).fit(X_red,y_red)
  y_predicho = clf.predict(X_test)
  evolucion_poly.append(f1_score(y_predicho,y_test,average='macro'))
  print('rbf,rbf')
  print(f1_score(y_predicho,y_test,average='macro'))
  clf= svm.SVC(kernel='linear').fit(X_red,y_red)
  y_predicho = clf.predict(X_test)
  print('linear,linear')
  print(f1_score(y_predicho,y_test,average='macro'))
  evolucion_linear.append(f1_score(y_predicho,y_test,average='macro'))

#500 elementos
#0.9736













""" > < , >= < , >=  <= , > <="""

## >= <= 
"""
import matplotlib.pyplot as plt
plt.scatter(parametro_gamma,evolucion_linear)
plt.scatter(parametro_gamma,evolucion_poly)
plt.axhline(t_lineal,color='blue')
plt.axhline(t_rbf,color='orange')
plt.axhline(fcnn_lineal,color='blue')
plt.axhline(fcnn_rbf,color='orange')
"""







